\chapter[Otimização multiobjetivo]{Otimização multiobjetivo}

A otimização multiobjetivo consiste em selecionar as melhores soluções de acordo com múltiplos critérios ao invés de apenas um. Por exemplo, ao estabelecer um melhor caminho entre duas cidades pode-se não estar interessado apenas na menor distância, mas também no tráfego, segurança das vias, quantidade de pedágios, etc. Na otimização com um único objetivo, a comparação entre soluções é relativamente simples. Para que uma solução seja considerada melhor que a outra, basta que ela tenha uma melhor avaliação segundo o critério/métrica considerada. Por outro lado, quando se trabalha com mais de uma função de otimização, é preciso usar um critério de ponderação, combinando as funções em um único objetivo, como, por exemplo, o algoritmo VEGA \cite{Schaffer1985}; ou adotar uma abordagem que trabalhe com o conceito de dominância de Pareto, como os algoritmos NSGA-II \cite{Deb2002} e SPEA2 \cite{Zitzler2002}.

A dominância de Pareto diz que uma solução $A$ é melhor que uma solução $B$, ou $A$ domina $B$ ($A \prec B$), se, e somente se:

\begin{itemize}  
	\item $A$ é melhor avaliado que $B$ em pelo menos um dos objetivos;
	\item $A$ não tem avaliação pior que $B$ em nenhum dos objetivos.
\end{itemize}

Considerando um problema de minimização e $F$ como o conjunto de funções objetivo, tem-se, matematicamente:

\[A \prec B \Leftrightarrow (\forall(f \in F) f(A) \leq f(B)) \land (\exists (f \in F) f(A) < f(B))\]

Em problemas de otimização multiobjetivo, o interesse está em encontrar a fronteira de Pareto, ou seja, o conjunto de todas as soluções que não são dominadas por nenhuma outra. Graficamente, a fronteira de Pareto representa a linha formada pelas soluções não-dominadas existentes para o problema. Na \autoref{fig_pareto} apresenta-se um exemplo da fronteira de Pareto para um problema de minimização com dois objetivos ($F1$ e $F2$). Nesse exemplo, os pontos pertencentes à fronteira de Pareto estão destacados em vermelho. Observe que, todas as soluções pertencentes à fronteira de Pareto são não dominadas, ou seja, nenhum ponto da fronteira possui ambos valores menores que alguma outra solução em ambas as funções ($F1$ e $F2$). Em contra partida, toda solução acima da fronteira (pontos em cinza) é dominada, pois existe alguma solução em vermelho que possui ambos valores de F1 e F2 menores. Caso o problema em questão fosse de maximização, a fronteira de Pareto estaria acima de qualquer solução não-dominada ao invés de abaixo.

\begin{figure}
	\centering
	\includegraphics[width=0.4\textwidth]{cap_otimizacao-multi/figs/pareto}
	\caption{\label{fig_pareto}Fronteira de Pareto}
\end{figure}

Não existe limite para o número de funções objetivo em um problema de otimização, mas quanto maior a quantidade de objetivos, mais complexa é a busca \cite{Deb2014}. Os algoritmos clássicos de otimização multiobjetivo \ac{NSGA-II} e \ac{SPEA2} lidam bem com até três objetivos, mas a partir de quatro critérios de otimização, ambos os métodos sofrem para encontrar soluções relevantes. Desta forma, criou-se a classificação ``\textit{many-objective}''. Problemas \textit{many-objective} (4 ou mais objetivos) apresentam um maior grau de dificuldade e precisam de novas técnicas para que sejam resolvidos eficientemente. Analisando as abordagens baseadas em algoritmos evolutivos multiobjetivos (AEMOs), \cite{Deb2014} observaram os seguintes os problemas trazidos pelo alto número de objetivos:

\begin{enumerate}  
	\item \textbf{Grande parte da população é não dominada:} a maioria dos algoritmos multiobjetivos classifica a população de acordo com a dominação de Pareto. Se existem muitas funções objetivo, é muito comum que uma solução seja melhor que outra em pelo menos uma delas. Desta forma, a maior parte das soluções se torna não-dominada, o que impede os algoritmos de evoluírem a população, já que todos os indivíduos são considerados igualmente bons.
	\item \textbf{Avaliar a diversidade da população se torna computacionalmente caro:} a fim de garantir uma boa diversidade populacional, os algoritmos adotam alguma medida de distância entre as soluções e removem aquelas consideradas mais similares. O aumento na dimensionalidade traz consequentemente um maior impacto (complexidade e custo) no cálculo da proximidade entre os indivíduos. 
	\item \textbf{\textit{Crossover} ineficiente:} a alta dimensionalidade do espaço de busca faz com que os indivíduos na população sejam muito distante uns dos outros e, normalmente, o cruzamento entre duas soluções muito diferentes resultam num filho muito distante dos pais, o que prejudica a convergência da busca. Portanto, pode ser necessário redefinir os operadores de recombinação a fim de restringir as possibilidades de pareamento.
	\item \textbf{População demasiadamente grande:} quanto maior o número de objetivos, maior o número de soluções na fronteira de Pareto. Portanto, para se obter bons resultados, é necessário que se manipule grandes populações de indivíduos, o que é computacionalmente caro e dificulta a análise do usuário que deverá escolher uma única solução ao final do processo.
	\item \textbf{Métricas de análise de desempenho do algoritmo se tornam difíceis de calcular:} a avaliação do resultado do algoritmo (taxa de erro, distância para o Pareto, hipervolume, etc.) está diretamente relacionada ao número de objetivos, quanto maior ele for, maior será o esforço computacional necessário. A complexidade do hipervolume, por exemplo, cresce exponencialmente com o número de objetivos.
	\item \textbf{Dificuldade de visualização:} é fácil representar graficamente as soluções e a fronteira de Pareto em problemas de até três objetivos. Entretanto, não existe uma forma eficiente de visualizar os resultados para problemas que lidam com maior dimensionalidade (4 objetivos em diante).
\end{enumerate}

A maior parte dos algoritmos \textit{many-objectives} mencionados neste trabalho lidam apenas com os quatro primeiros problemas. As duas últimas não são responsabilidade dos algoritmos de otimização em si.

\section{Algoritmos Bio-Inspirados Multiobjetivos}

A maior parte dos métodos de busca multiobjetivo são baseados em algoritmos inspirados na biologia. Esses algoritmos são adaptações das estratégias evolutivas, colônias de formigas e enxames de partículas tradicionais para lidar com problemas que envolvam mais de um objetivo. A principal diferença entre um algoritmo tradicional e sua variação multiobjetivo está na forma de cálculo da aptidão dos indivíduos da população. No caso dos algoritmos multiobjetivos, geralmente o conceito de dominância é usado de diferentes formas \cite{Bueno2010}.

A seguir são apresentados alguns algoritmos bio-inspirados multiobjetivo encontrados na literatura. Os dois primeiros (NSGA-II e SPEA2) são algoritmos bem conhecidos e largamente utilizados em problemas multiobjetivos. Apesar de sua eficiência na resolução de problemas com até 3 objetivos, o desempenho desses AEMOs costuma cair consideravelmente com o aumento no número de objetivos \cite{Franca2017}. Os demais algoritmos apresentados, são mais recentes e foram concebidos especificamente para tratar de problemas \textit{many-objective}, ou seja, que envolvam 4 ou mais objetivos.

\subsection{NSGA-II}
O \textit{Non-dominated Sorting Genetic Algorithm II} (NSGA-II) \cite{Deb2002} é o algoritmo evolutivo multiobjetivo mais frequente na literatura. O processo desse algoritmo é semelhante ao do algoritmo genético comum, com diferença no cálculo de aptidão, que é feito por \textit{ranks}, e no cálculo de distâncias, que é inexistente na proposta original do AG. A atribuição de aptidão (\textit{fitness}) se dá pela classificação da população em categorias/classes (\textit{rankings}) de dominância (fronteiras), de forma que o primeiro contenha todas as soluções não dominadas, o segundo todos os indivíduos não-dominados excluindo aqueles presentes na primeira fronteira, e assim por diante. Quanto melhor o \textit{ranking} de uma solução, melhor sua aptidão e  maior sua chance de sobreviver para a próxima geração. Várias soluções podem pertencer a uma mesma fronteira. A fim de diferenciá-las entre si, é utilizado um cálculo de distância (\textit{crowding distance}), o qual confere melhor avaliação às soluções mais diferentes (maior distância), garantindo assim a diversidade da população.

O fluxo de funcionamento do NSGA-II inicia-se com a geração aleatória dos indivíduos. Em seguida, a população é classificada em \textit{ranks} de dominância e inicia-se o processo iterativo, o qual termina assim que a condição de parada é atingida. As etapas que compõem cada iteração do NSGA-II são descritas no pseudo-código do algoritmo \ref{alg_nsgaii}.

\begin{algorithm}
	\caption{Processo iterativo do NSGA-II}
	\label{alg_nsgaii}
	\begin{algorithmic}[1]
		\While {critério de parada não for atingido}
		\State $Pais \gets selecao(Pop_{atual}, TX_{cross})$
		\State $Filhos \gets crossover(Pais)$
		\State $Pop_{nova} \gets Pop_{atual} \cup Filhos$
		\State $ranks \gets ranking(P_{nova})$
		\State $crowdingDistance(P_{nova})$
		\State $Pop_{atual} \gets reinsecao(ranks, tam_{populacao})$
		\EndWhile
	\end{algorithmic}
\end{algorithm}

O processo iterativo do algoritmo inicia-se com a seleção dos pares de pais para o cruzamento (linha 2). Na implementação considerada, a seleção de pais utiliza torneio simples com \textit{tour} de 2, ou seja, dois elementos da população são escolhidos de forma aleatória e o indivíduo com melhor avaliação é selecionado como um dos pais. Então, esse processo é repetido para a escolha do segundo pai.

Na linha 3 do pseudo-código, através dos pares de pais, gera-se os filhos com o \textit{crossover} e a mutação. Após a geração dos filhos, a população corrente e o conjunto de filhos são concatenados (linha 4) e submetidos à classificação em \textit{ranks} de dominância (linha 5).

A classificação em \textit{ranks} de dominância recebe um conjunto de soluções e verifica quais dentre elas não são dominadas. O conjunto de soluções não-dominadas forma o primeiro \textit{rank} de dominância. Do conjunto restante (excluindo o primeiro \textit{rank}), retira-se as soluções não dominadas para formar o segundo \textit{rank}. Esse processo se repete até que todos os indivíduos tenham sido classificados.

Após toda a população ter sido classificada em \textit{ranks}, na linha 6, o algoritmo calcula a distância de aglomeração (\textit{crowding distance}) dos indivíduos em cada \textit{rank} de dominância. O cálculo da distância, considerando cada objetivo avaliado, ordena o conjunto de soluções e faz uma relação entre as distâncias de cada indivíduo para os vizinhos adjacentes (que estão imediatamente antes ou depois). A distância de aglomeração de cada indivíduo é resultado da somatória das distâncias resultantes em cada objetivo.  Quanto maior o valor, maior é a diferença entre o indivíduo e seus pares dentro do \textit{rank}. Essa métrica é usada para manter a diversidade das soluções entre as gerações do algoritmo.

Com toda a população classificada em \textit{ranks} (ou fronteiras) e todas as distâncias calculadas, na linha 7, uma nova população é escolhida com base nos melhores indivíduos entre pais e filhos. Para isso, analisa-se fronteira a fronteira, da melhor para a pior, até que o tamanho máximo da população seja atingido. Para cada fronteira aplica-se o seguinte processo de decisão:

\begin{itemize}  
	\item Se $tamanho(rank) + tamanho(novaPopulação) < tamPop$: adiciona-se todos os membros do \textit{rank} à nova população.
	\item Caso contrário: adiciona-se à nova população os ($tamPop - tamanho(novaPopulação)$) elementos do \textit{rank} com os maiores valores de distância.
\end{itemize}

Desta forma, ao final do algoritmo obtém-se a fronteira de Pareto aproximada na primeira fronteira da população gerada na última iteração do algoritmo.

\subsection{SPEA2}

O \textit{Strength Pareto evolutionary algorithm 2} (SPEA2) \cite{Zitzler2002} é um AEMO que calcula, para cada membro da população, sua força (\textit{strength}) e densidade. A força de uma solução é dada pelo número de indivíduos que ela domina, enquanto a densidade é uma medida de distância para os vizinhos mais próximos, quanto maior a densidade mais próximo o indivíduo está das demais soluções. A aptidão (\textit{fitness}) de uma solução é definida por sua densidade mais a soma das forças de todo indivíduo que a domina. 

Os indivíduos mais aptos no SPEA2 são aqueles dominados pela menor quantidade de soluções e que possuem maior variabilidade genética. O algoritmo calcula a aptidão em três etapas: cálculo de força (\textit{strength}), da aptidão crua (\textit{raw fitness}) e da densidade \cite{Zitzler2002}.

A força de um indivíduo $i$ ($s(i)$) é o número de soluções que ele domina, ou seja, considerando $A$ o arquivo e $P$ a população:

\[ s(i) = |j|: j \in P \cup A \land i \prec j \]

Uma vez calculada a força de cada indivíduo, parte-se para o cálculo do \textit{raw fitness}. O \textit{raw fitness} de um indivíduo $i$ ($r(i)$) é dado pela soma das forças de cada elemento que o domina, como segue:

\[ r(i) = \sum_{j \in A \cup P | j \prec i} s(j) \]

Observe que, caso o indivíduo seja não-dominado, seu \textit{raw fitness} será o menor possível (zero). Para finalizar o cálculo de aptidão, é definida a densidade de cada indivíduo ($d(i)$). Essa densidade é computada de acordo com a distância da solução para seus vizinhos e é dada por:

\[ d(i) = \frac{1}{\sigma_i^k + 2} \]

Sendo, $\sigma_i^k$ a $k$-ésima menor distância entre o indivíduo $i$ e o restante da população; e $k$ a raiz quadrada do tamanho do conjunto de soluções em avaliação, i.e. $k = \sqrt{|P \cup A|}$. O valor de $d(i)$ sempre está no intervalo (0,1).

Finalmente, a aptidão do indivíduo ($f(i)$) é dada pela soma do \textit{raw fitness} e a densidade: 
\[f(i) = r(i) + d(i)\]

Note que, se a solução $i$ é não-dominada, $f(i) < 1$. Isso ocorre dado que $d(i) < 1$ para qualquer solução e, quando $i$ é não-dominada, $r(i) = 0$.


Além do cálculo de aptidão, outra diferença importante entre o SPEA2 e um AG tradicional é a utilização de uma população extra, denominada arquivo. O arquivo é responsável por guardar as melhores soluções já encontradas até o momento, funciona como uma espécie de elitismo. Na seleção para o cruzamento, os pais são sempre escolhidos do arquivo e os filhos substituem 100\% da população corrente. A cada iteração, os melhores indivíduos entre a população e o arquivo compõe o arquivo da geração seguinte. A quantidade de indivíduos no repositório de soluções não-dominadas é limitada e, quando esse tamanho máximo ($tam_arq$) é excedido, deve-se executar um processo de truncamento.

O processo de truncamento de um arquivo ocorre na seleção natural (reinserção), que é a última função executada na iteração do laço principal de um AG. No SPEA2, a reinserção consiste em definir o arquivo para a próxima geração a partir das populações. Nesse processo, extrai-se do conjunto total de soluções (população e arquivo) aquelas que não são dominadas por nenhuma outra, e com esse subconjunto ($n_d$) constrói-se o novo arquivo através do seguinte processo de decisão:

\begin{itemize}  
	\item Se $tamanho(n_d) = tam_arq$, o novo arquivo é formado por $n_d$;
	\item Se $tamanho(n_d) < tam_arq$, o novo arquivo é formado pela união de $n_d$ com os $tam_arq - tamanho(n_d)$ indivíduos restantes com melhor aptidão;
	\item Caso contrário, ($tamanho(n_d) > tam_arq$), o novo arquivo é formado por $n_d$ e deve-se truncá-lo em ($tamanho(n_d) - tam_arq$) passos, onde em cada passo, elimina-se o indivíduo com menor variabilidade genética em relação aos demais.
\end{itemize}

O algoritmo retorna como resposta para o problema o arquivo resultante da última geração computada. Espera-se que, após as diversas iterações, o algoritmo tenha conseguido uma boa aproximação da fronteira de Pareto.

%O laço principal do SPEA2 é explicitado no algoritmo \ref{alg_spea2} e, como resposta para o problema, retorna-se o arquivo resultante da última geração computada. Espera-se que, após as diversas iterações, o algoritmo tenha conseguido uma boa aproximação da fronteira de Pareto.

%\begin{algorithm}
%	\caption{Laço principal do SPEA2}
%	\label{alg_spea2}
%	\begin{algorithmic}[1]
%		\While {número máximo de gerações não for atingido}
%		\State a partir do arquivo, selecione os pares de pais para o crossover
%		\State efetue o cruzamento para cada par de pais, gerando os filhos
%		\State substitua a população corrente pelos filhos
%		\State calcule o fitness de todos indivíduos no arquivo e na população
%		\State aplique a seleção natural e trunque o arquivo, caso necessário
%		\EndWhile
%	\end{algorithmic}
%\end{algorithm}

\subsection{MOEA/D}
\label{section_moead}

O \textit{Multiobjective evolutionary algorithm based on decomposition} (MOEA/D) \cite{Zhang2007} é um algoritmo que avalia os objetivos através de uma função escalarizadora, se baseando na dominância de Pareto apenas para atualizar o conjunto de soluções não dominadas geradas em cada iteração (arquivo). No MOEA/D, um problema multiobjetivo é decomposto em múltiplos problemas de um único objetivo chamados de células. Cada célula é definida por um vetor de pesos gerado aleatoriamente e representa um indivíduo, ou seja, o número de células é igual ao tamanho da população. Além dos pesos, a célula, ou indivíduo, é composta de uma solução e uma vizinhança. A vizinhança é formada pelos $k$ indivíduos mais próximos de acordo com o vetor de pesos, onde $k$ é um parâmetro do algoritmo que representa o tamanho das vizinhanças. A aptidão (\textit{fitness}) de uma solução é calculada de acordo com sua avaliação em cada objetivo, a função escalarizadora, e o vetor de pesos da célula. Em toda geração, uma nova solução é gerada para cada célula, onde a vizinhança é considerada nas etapas de escolha dos pais e seleção natural.

O início do algoritmo consiste na geração da estrutura de células e na definição das vizinhanças. Para isso, sorteia-se os vetores de pesos (a soma de cada vetor deve ser igual a um) e, para cada um deles, calcula-se os $k$ vetores mais próximos (vizinhança). Essa estrutura é imutável e é utilizada no decorrer de todo o algoritmo. A geração dos vetores de pesos pode ser tanto aleatória quanto seguir uma distribuição pré-definida. Antes de começar o processo iterativo (evolução das gerações), gera-se aleatoriamente uma solução para cada célula e calcula-se a sua aptidão. 

Uma parte fundamental do MOEA/D é a escolha da função escalarizadora, a qual é a principal responsável pelo cálculo de aptidão. Em todos experimentos realizados neste trabalho, foi utilizada a soma ponderada, mas outras estratégia, como \textit{Penalty-Based Boundary Intersection} e Tchebycheff também podem ser utilizadas \cite{Zhang2007}. A aptidão de uma solução é calculada através da função escalarizadora e do vetor de pesos. Por exemplo, se os valores $[2, 9, 5]$ representam a solução $s$ no espaço de objetivos, $[0.3, 0.2, 0.5]$ é o vetor de pesos da célula $c$, e a soma ponderada é a função escalarizadora, então a aptidão de $s$ em $c$ é dada por $2 \times 0.3 + 9 \times 0.2 + 5 \times 0.5 = 4.9$.

Como em qualquer outro AG, o processo evolutivo do MOEA/D consiste basicamente da seleção de pais, da geração dos filhos e sua reinserção na população. Para cada célula $c_i$, dois pais são selecionados aleatoriamente em sua vizinhança. Quando um filho é gerado para um célula $c_i$, sua aptidão é calculada para cada uma das células na vizinhança de $c_i$, substituindo a solução atual caso o \textit{fitness} do novo indivíduo (filho) seja melhor. Ao final de cada iteração (geração), atualiza-se o arquivo com as novas soluções não-dominadas.

\subsection{NSGA-III}

O \textit{Non-dominated Sorting Genetic Algorithm III} (NSGA-III) \cite{Deb2014} é uma extensão do NSGA-II que permite o \textit{framework} funcionar melhor para mais de três objetivos (many-objective). Ele se diferencia do original apenas na fase de reinserção ou seleção natural, onde ao invés de usar a distância de aglomeração para diferenciar soluções em uma mesma fronteira, utiliza um método de agrupamento, no qual os indivíduos são divididos em nichos de acordo com suas similaridades. O NSGA-III é caracterizado pelo processo de atribuição de nicho chamado de classificação não-dominada baseada em pontos de referência. Sua ideia é traçar uma figura geométrica de uma dimensão a menos que o número de objetivos nos pontos extremos da primeira fronteira. Um número pré-definido de pontos de referência equidistantes são distribuídos ao longo da figura, cada qual representando um respectivo nicho. Para classificar uma solução, define-se como nicho o ponto de referência mais próximo. Ao final, são selecionadas (sobrevivem) as soluções localizados nas regiões menos lotadas do espaço de busca. Maiores detalhes sobre esse processo de agrupamento podem ser obtidos no artigo original do algoritmo \cite{Deb2014}.

\subsection{SPEA2-SDE}

O \textit{SPEA2 with Shift-Based Density Estimation} (SPEA2-SDE) \cite{Spea2SDE} é uma pequena alteração no algoritmo SPEA2 que possibilita lidar com problemas que envolvam 4 ou mais objetivos de uma forma muito mais eficiente. A única alteração está no cálculo de distância entre duas soluções. Suponha que $s_1$ e $s_2$ sejam dois indivíduos na população e que seus valores no espaço de objetivo sejam, respectivamente, $[5, 10, 351, 7, 15]$ e $[6, 8, 15, 9, 14]$. No SPEA2, a distância entre $s_1$ e $s_2$ é dada pela distância euclidiana dos dois vetores ($dist$), ou seja:

\[dist(s_1, s_2) = \sqrt{(5-6)^2 + (10-8)^2 + (351-15)^2 + (7-9)^2 + (15-14)^2} = 336,0148\]

As duas soluções são bem próximas, pois só estão distantes em uma das 5 coordenadas. Entretanto, o cálculo de distância do SPEA2 não reflete isso, o que é prejudicial em problemas com muitos objetivos. A fim de resolver esse problema, \cite{Spea2SDE} introduziram o cálculo de distância \ac{SDE}, que nada mais faz do que transladar a coordenada mais distante do segundo ponto para o mesmo valor no primeiro ponto. Isto é, antes de calcular a distância euclidiana, identifica-se a coordenada que exibe maior diferença entre os dois pontos. No caso de $s_1$ e $s_2$, a terceira coordenada é a que possui esse comportamento. Em seguida, é realizada a translação do segundo ponto na coordenada identificada para que seja igual ao primeiro ponto, no exemplo, ($s_2' = [6, 8, \textbf{351}, 9, 14]$). A distância SDE é, então, determinada pela distância euclidiana entre o primeiro ponto ($s_1$) e o segundo ponto transladado ($s_2'$). No exemplo, a distância SDE obtida é: 

\[dist_{SDE}(s_1, s_2') = \sqrt{(5-6)^2 + (10-8)^2 + (351-351)^2 + (7-9)^2 + (15-14)^2} = 3,1622\]

A distância SDE descarta a coordenada mais distante ao calcular as distâncias, permitindo assim que pontos distantes em apenas uma coordenada ainda sejam considerados próximos. Todo o restante do processo do SPEA2-SDE segue os mesmos passos do algoritmo original.

\subsection{AEMMT}

O Algoritmo Evolutivo Multiobjetivo com Muitas Tabelas (AEMMT) \cite{Brasil2013}, assim como o MOEA/D, decompõe o problema multiobjetivo em subproblemas menores de um único objetivo. Entretanto, o AEMMT utiliza um esquema de tabelas, onde cada tabela representa uma combinação diferente dos objetivos investigados. A função que transforma os múltiplos objetivos em um valor escalar é a média aritmética e cada tabela mantém os melhores indivíduos, considerando o valor médio dos objetivos que ela representa. A cada geração, duas tabelas são selecionadas para o cruzamento. Dois pais, um de cada tabela, são sorteados aleatoriamente para gerarem um único filho. Essa nova solução (filho) é avaliada em todas as tabelas, entrando naquelas em que representar uma melhor aptidão em relação aos indivíduos atuais. Naturalmente, como um único \textit{crossover} é realizado e um único filho é gerado a cada iteração, o AEMMT precisa de mais gerações para alcançar o mesmo número de soluções avaliadas que os algoritmos citados anteriormente.

A quantidade de tabelas é determinada pelo número de combinações possíveis entre os objetivos. Para quatro objetivos ($f_1, f_2, f_3, f_4$), por exemplo, serão criadas 15 tabelas de combinações mais uma tabela extra, usada para guardar os indivíduos não-dominados, como ilustrado na \autoref{fig_aemmt_tabelas}. Cada tabela possui um limite máximo de indivíduos. No início do algoritmo, gera-se soluções aleatórias de forma que todas as tabelas sejam completamente preenchidas. No laço principal, um indivíduo só entra em uma tabela $t$, se for melhor que a pior solução na população atual de $t$. Com relação a tabela de dominância, sempre que um filho é gerado e não-dominado por nenhum outro indivíduo da tabela, ele é incluído. A restrição no tamanho da tabela de dominância é independente das demais e, sempre que o limite for atingido, é feito um truncamento priorizando a permanência das soluções com maior valor de média aritmética entre todos os objetivos.

\begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{cap_otimizacao-multi/figs/aeemt-tabelas}
	\caption{\label{fig_aemmt_tabelas}Exemplo da quantidade de tabelas usadas pelo AEMMT}
\end{figure}

Após a geração e preenchimento das tabelas, inicia-se o laço principal. A cada iteração são escolhidas duas tabelas via torneio duplo de acordo com suas pontuações. A pontuação tem valor inicial zero e sempre que uma tabela gera um filho que sobrevive para a geração seguinte, sua pontuação é incrementada. As pontuações são zeradas a cada 100 gerações. Considerando as duas tabelas selecionadas, sorteia-se um indivíduo de cada e efetua-se o cruzamento entre eles. O filho gerado é, então, comparado tabela à tabela, entrando naquelas em que representar uma melhoria. Após a execução de todas as gerações, dá-se como resultado o conjunto não dominado, considerando as soluções de todas as tabelas.

\subsection{AEMMD}
\label{section_aemmd}

O Algoritmo Evolutivo Multiobjetivo com Múltiplas Dominâncias (AEMMD) \cite{Lafeta2016} é uma modificação do AEMMT que, apesar de usar o mesmo processo de divisão do problema multiobjetivo, abandona a ideia de escalarização e adota o conceito de dominância também usado métodos mais antigos (e.g. NSGA-II e SPEA2). No AEMMD, ao invés de se utilizar a média dos objetivos da tabela para avaliar o indivíduo, lança-se mão da relação de dominância de Pareto. Um indivíduo novo $s$ só entra na tabela $t$, se $s$ não for dominado por nenhuma solução em $t$ considerando apenas os objetivos da tabela. Além disso, se $s$ entra em $t$, todas as soluções em $t$ dominadas por $s$ são removidas.

O primeiro passo do AEMMD é gerar o conjunto de tabelas, considerando todas as combinações de objetivos possíveis a partir de dois a dois. Combinações de um único objetivo não são criadas, pois o conceito de dominância é válido a partir de dois valores. Diferentemente do AEMMT, as tabelas não possuem limite de tamanho e podem crescer indefinidamente. Por exemplo, para um problema de quatro objetivos ($f_1, f_2, f_3, f_4$), 11 tabelas seriam geradas, como pode ser observado na Figura \ref{fig_aemmd_tabelas}.

\begin{figure}
	\label{fig_aemmd_tabelas}
	\centering
	\includegraphics[width=1\textwidth]{cap_otimizacao-multi/figs/aeemd-tabelas}
	\caption{Exemplo da quantidade de tabelas usadas pelo AEMMD}
\end{figure}

Com as tabelas criadas, gera-se um número pré-definido de soluções aleatórias, distribuindo-nas pelas tabelas de acordo com a relação de dominância de Pareto. Assim como no AEMMT, a cada iteração do algoritmo, é utilizado um torneio duplo para selecionar duas tabelas de acordo com suas pontuações. Entretanto, a pontuação das tabelas no AEMMD é diferente do AEMMT. Ao invés de conceder um ponto sempre que se gera um indivíduo sobrevivente, pontua-se uma tabela quando ela recebe um indivíduo. Tendo escolhido as duas tabelas, sorteia-se um representante de cada e gera-se um único filho, o qual é comparado tabela à tabela, entrando naquelas onde representa uma solução não dominada. Outra diferença em relação ao método original, é que o AEMMD não reinicia as pontuações em momento algum do algoritmo. Espera-se que ao final das gerações, a população da tabela com todos os objetivos tenha convergido para a fronteira de Pareto. 

\section{Algoritmos multiobjetivos baseados em colônias de formigas}

A maior parte dos métodos de busca multiobjetivo são baseados em algoritmos genéticos. Entretanto, uma alternativa que ainda é pouco explorada são as técnicas inspiradas em inteligência coletiva. Dentre elas, optou-se por investigar métodos de otimização baseados em colônias de formigas, os quais são particularmente adequados para lidar com problemas discretos, como aqueles utilizados em neste trabalho (problema da mochila multiobjetivo e problema do roteamento multicast). Vários ACOs multiobjetivos são encontrados na literatura, dentre eles destacam-se o MOACS \cite{Baran2003} e o MOEA/D-ACO \cite{Ke2013}.

\subsection{MOACS}

O \textit{Multi-Objective Ant Colony Optimization Algorithm} (MOACS) \cite{Baran2003} é uma adaptação do ACO original que torna possível a otimização de múltiplos objetivos utilizando uma única estrutura de feromônios, múltiplas heurísticas e um arquivo de soluções não dominadas. Esse algoritmo foi proposto originalmente para o problema de roteamento de veículos com janelas de tempo e, posteriormente, foi aplicado no problema do roteamento multicast \cite{Pinto2005}. Uma variação do algoritmo original foi apresentada em \cite{Riveros2016}, a qual é utilizada neste trabalho. Seu funcionamento básico é descrito no Algoritmo \ref{alg_moacs}.

\begin{algorithm}
	\caption{Algoritmo MOACS}
	\label{alg_moacs}
	\begin{algorithmic}[1]
		\State Inicialize a estrutura de feromônios $\tau_{ij}$ com $\tau_0$ /* $\tau_{0}$ é o valor inicial */
		\State Crie um conjunto vazio de soluções não-dominadas $ND$
		\While {Número máximo de iterações não for atingido}
		\For {$i \gets 0$ até $tamanho\_populacao$}
		\State Sorteie valores no intervalo $[0, w_{max}[$ para formar um vetor de pesos $W$ com $|H|$ posições
		\State Construa uma solução de acordo com a tabela de feromônios $\tau_{ij}$, as heurísticas ($H$) e os pesos $W$
		\State Atualize $ND$ com a nova solução
		\EndFor
		\If {$ND$ foi modificado}
		\State Reinicie a estrutura de feromônios fazendo $\tau_{ij} = \tau_0 \forall(i,j)$
		\Else
		\State Atualize a estrutura de feromônios com todas as soluções em $ND$
		\EndIf
		\EndWhile
		\State \Return $ND$
	\end{algorithmic}
\end{algorithm}

O processo de construção de uma solução depende do problema investigado. No MOACS, os principais componentes utilizados nesse processo são:

\begin{itemize}  
	\item Feromônios ($\tau_{ij}$): estrutura que guarda a quantidade de feromônios em cada partícula que pode formar a solução. No caso de problemas em grafos, representa a quantidade da substância em cada uma das arestas, indicando a frequência de suas utilizações;
	\item Heurísticas ($H$): conjunto de funções que estimam a qualidade de uma dada partícula que pode formar a solução. No caso de problemas em grafos, representa os vários pesos em uma aresta. Por exemplo, num grafo que representa uma rede de computadores com informações de custo, distância e tráfego, $H$ poderia ser formado de três funções que recebem uma aresta $(i,j)$ e devolvem, respectivamente, os valores de suas métricas custo, distância e tráfego.
	\item Peso máximo de uma heurística ($w_{max}$): representa o valor máximo que o peso de uma heurística pode atingir. Em \cite{Riveros2016}, propõe-se $w_{max} = 3$, de forma que cada função possa ser classificada como 0 (não importante), 1 (importante), 2 (muito importante).
	\item Vetor de pesos ($W$): O vetor de pesos atribui a importância de cada heurística e normalmente é gerado de forma aleatória em cada iteração. Cada função de heurística recebe um peso variando no intervalo $[0, w_{max}[$.
\end{itemize}

Ao construir uma solução, utiliza-se o mesmo processo de decisão do ACO original, explicado na seção \ref{section_construcao_solucao}. A única diferença é que as múltiplas heurísticas do MOACS ($H$) são unificadas em uma única função $h(x)$, por meio de uma média ponderada. Nesse processo, utiliza-se o vetor de pesos $W$ para definir o fator de ponderação de cada heurística. Dessa forma, a função h(x) é dada por:

\[h(x) = \frac{\sum_{i \gets 0}^{size(H)}\ H_i(x) \times W_i}{\sum_{w \in W} w}\]

Em cada época (iteração do ACO), atualiza-se o arquivo de soluções não dominadas com as novas soluções geradas. Se o arquivo foi atualizado após criar-se todas as soluções, reinicia-se as informações de feromônio, redefinindo todos os valores na estrutura $\tau_{ij}$ para o valor inicial de feromônio $\tau_0$. Caso o arquivo tenha se mantido estável, ou seja, nenhuma das novas soluções é não-dominada, atualiza-se as quantidades de feromônio na estrutura de acordo com as soluções no arquivo.

Considerando um problema em grafos, para atualizar a estrutura $\tau_{ij}$ com uma solução $s$, faz-se:

\[\tau_{ij} = (1 - \rho) \times \tau_{ij} + \rho \times \Delta\tau(s) \qquad \forall(i,j) \in s\]

Sendo:
\begin{itemize} 
	\item $\rho$: coeficiente de evaporação;
	\item $\Delta\tau(s)$: Quantidade de feromônios depositados pela solução $s$, que por sua vez é definida por:
\end{itemize}

\[\Delta\tau(s) = \frac{1}{performance(s)}\]

Na fórmula anterior, $performance(s)$ é dado pela soma dos valores de $s$ no espaço de objetivos. Neste caso, considera-se um problema de minimização. Por exemplo, se os objetivos são reduzir o custo, o tráfego e o atraso (\textit{delay}) de uma rede, então, $performance(s) = custo(s) + trafego(s) + delay(s)$. Para problemas de maximização, basta inverter a equação.

Após todas as iterações do MOACS, espera-se obter no arquivo uma boa aproximação da fronteira de Pareto. 

\subsection{MOEA/D-ACO}
O \textit{Multiobjective evolutionary algorithm based on decomposition ACO} (MOEA/D-ACO) \cite{Ke2013} é um método que adota o algoritmo MOEA/D (seção \ref{section_moead}) aplicado ao \textit{framework} de otimização por colônia de formigas (ACO). Os conceitos de células e vizinhanças são reutilizados, enquanto que a reprodução local, inexistente no ACO, é substituída por um processo de construção da solução levemente modificado. Além disso esse método introduz um novo conceito de grupos que é utilizado para agrupar as formigas de acordo com seus vetores de peso. Nesse algoritmo, o termo formiga corresponde ao conceito de célula do MOEA/D.

O primeiro passo do MOEA/D-ACO consiste em gerar os vetores de peso que, assim como no MOEA/D, são arranjos de valores entre 0 e 1, cujas somas valem 1. A geração pode ser aleatória ou seguir alguma distribuição pré-definida. No caso deste trabalho, utilizou-se a distribuição uniforme proposta no artigo original \cite{Ke2013}. Atribui-se cada vetor a uma formiga e cria-se a estrutura de vizinhanças, onde cada formiga é associada a uma vizinhança contendo as $v_{size}$ formigas mais próximas de acordo com os vetores de peso (incluindo ela mesma). O segundo passo do algoritmo determina os grupos, cada grupo recebe um novo vetor de pesos gerado da mesma maneira que os pesos anteriores. As formigas são então distribuídas entre os grupos de acordo com a proximidade entre seu vetor de pesos e o vetor de pesos do grupo (\textit{agrupamento}).

Com respeito às duas principais estruturas de um ACO, heurística e feromônios, cada formiga possui uma função heurística e cada grupo mantém uma estrutura de feromônios. Quando os grupos são criados, cada um recebe uma estrutura de feromônios com o valor máximo de feromônio em cada uma das posições. Por sua vez, quando a formiga é criada, recebe uma função heurística baseada na combinação de todas as funções heurísticas do problema e no vetor de pesos da própria formiga. O cálculo específico da heurística depende do problema.

O processo iterativo (evolução) do MOEA/D-ACO consiste em gerar as soluções, atualizar o conjunto de soluções não-dominadas $ND$, distribuir as novas soluções entre as formigas e atualizar as estruturas de feromônios. Primeiramente, para cada grupo $G$, gera-se as soluções para cada formiga $f \in G$. A solução é gerada com base nos feromônios de $G$, na heurística de $f$ e na solução atual de $f$. Tendo gerado todas as soluções para um grupo, atualiza-se o arquivo de soluções não-dominadas $ND$. Para cada nova solução $S$ que entrou no arquivo, modifica-se a estrutura de feromônios $\tau$ de $G$. Assim, o cálculo do feromônio de uma partícula $e$ na iteração $i+1$ ($\tau_{i+1}(e)$) é obtida a partir do seu feromônio atual ($\tau_i(e)$), como segue:

\[
\tau_{i+1}(e)= 
\begin{cases}
\rho \times \tau_i(e) + \delta,& \text{se } e \in S\\
\rho \times \tau_i(e),              & \text{caso contrário}
\end{cases}
\]

\vspace{4mm} %4mm vertical space

Onde $e$ é uma possível partícula de uma solução para o problema investigado (qualquer aresta, PRM, ou item, PMM) da tabela de feromônios; $\rho$ é a taxa de evaporação; e $\delta$ é dado pelo inverso da soma dos valores da função de escalarização aplicada sobre cada solução não dominada de $G$ em relação ao vetor de pesos de $G$.

\[\delta = \frac{1}{\sum_{s \in S_{nd}f(s, W)}}\]
	
Sendo, $S_{nd}$ o conjunto de soluções não dominadas de $G$, $f$ a função de escalarização, e $W$ o vetor pesos de $G$. Caso o problema seja de maximização, $\delta$ seria a soma dos valores, ao invés do inverso dela. 

Cada formiga armazena duas soluções, a solução atual, que de início é nula, e a nova solução, criada em cada iteração do algoritmo. Após a geração de novas soluções a atualização de feromônios para todos os grupos e suas formigas, deve-se decidir a solução atual de cada formiga com base nas novas soluções da vizinhança. Nesse processo, para cada formiga $f$ analisa-se as novas soluções presentes na vizinhança de $f$. Se alguma nova solução $sn$, que ainda não substituiu nenhuma outra, possui um \textit{fitness} melhor que o da solução corrente de $f$ ($sc$), substitui-se $sc$ por $sn$. O \textit{fitness} é calculado de acordo com a média ponderada dos valores da solução em cada objetivo baseadas no vetor de pesos da formiga em questão. Neste trabalho, utilizou-se a soma ponderada como função \textit{escalarizadora}, mas qualquer uma das funções propostas em \cite{Zhang2007} pode ser usada.

O processo de construção da solução depende do problema investigado. Entretanto, o MOEA/D-ACO inclui duas novas características no processo, independente do problema. São elas:

\begin{itemize}
	\item \textbf{Influência da solução atual:} no MOEA/D-ACO, cada formiga mantém uma solução atual que influencia a construção da próxima solução com base em um parâmetro $\delta$. Isso acontece devido à introdução de um novo termo no cálculo de feromônio na construção da solução. Ao calcular a probabilidade de uma partícula $p$ (aresta ou item) fazer parte da solução, ao invés de adotar $feromonio(p)^\alpha$, considera-se $(\delta * x + feromonio(p))^\alpha$, sendo $x = 1$ se $p$ pertence à solução atual e $x = 0$ caso contrário.
	\item \textbf{Taxa de elitismo:} a maioria dos ACOs utilizam uma espécie de roleta para decidir qual partícula fará parte da solução. Aquelas com maior probabilidade têm maior chance de serem escolhidas, mas não necessariamente serão. Em uma estratégia elitista, não se utiliza a roleta. A partícula escolhida será aquela que receber o maior valor de probabilidade. No MOEA/D-ACO, utiliza-se uma taxa de elitismo que estipula a chance de uma partícula ser escolhida por elitismo ao invés de roleta.
\end{itemize}

O processo iterativo do algoritmo é executado até que uma condição de parada pré-definida seja atingida. Normalmente adota-se um número máximo de iterações. Espera-se que, nesse momento, as soluções no conjunto $ND$ tenham convergido para a fronteira de Pareto.

\section{Outros algoritmos multiobjetivos}
Alguns outros algoritmos multiobjetivos não foram analisados neste trabalho, mas por estarem relacionados ao tema da pesquisa, são mencionados a seguir.

\ac{VEGA} \cite{Schaffer1985}, \ac{NSGA} \cite{Srinivas1994} e \ac{SPEA} \cite{Zitzler1999} foram os primeiros algoritmos multiobjetivos a aparecerem na literatura, apesar do NSGA-II e SPEA2 serem os mais conhecidos. Em problemas \textit{many-objective}, uma das principais dificuldades encontradas é o excesso de soluções não-dominadas, o que dificulta a identificação de melhores indivíduos e prejudica a convergência. Para lidar com esse problema, \cite{Aguirre2009} apresentam um conceito menos rigoroso de dominância, dando origem ao algoritmo $\epsilon$-MOEA. Por sua vez, a fim de reclassificar as soluções não dominadas e resolver o mesmo problema, \cite{Beume2007} propõem o algoritmo SMS-EMOA, no qual a evolução da população ocorre de acordo com indicadores de qualidade do conjunto de soluções. Nesse algoritmo, os autores usam o hipervolume para avaliar soluções consideradas igualmente boas pela dominância de Pareto. \cite{Bader2011} utilizam uma ideia parecida ao propor o algoritmo Hype. Esse algoritmo também utiliza o hiper-volume como parâmetro para guiar a evolução da população. 

%Em \cite{Ishibuchi2015} Ishibuchi apresenta uma comparação entre os principais métodos de otimização aplicados ao PMM e em \cite{Franca2017}, artigo resultante deste trabalho, estende-se a comparação para métodos mais recentes (como o AEMMT) e um novo problema discreto, o PRM.

%Sobre os algoritmos de otimização multiobjetivos baseados em colônias de formigas, em 2004 Alaya et al. aplica o \textit{MIN-MAX Ant System} no problema da mochila com múltiplos objetivos e múltiplas restrições \cite{Alaya2004}. Outras aplicações de ACOs no problema da mochila podem ser encontradas em \cite{changdar2013,Ke2010,Fingler2014,kong2008,Fidanova2003}. 

\cite{SouzaPozo2015} propõem o algoritmo MOEA/D-BACO que aplica uma variação do MOEA/D-ACO no problema de programação binária quadrática irrestrito (UBQP). Como para o UBQP as estruturas de feromônio crescem de forma exponencial em relação ao tamanho da entrada, torna-se inviável. A variação proposta pelos autores, algoritmo MOEA/D-BACO, visa solucionar esse problema através da substituição do ACO, no MOEA/D-ACO, pelo \ac{BACO} \cite{baco2006}, que é um algoritmo mais simples e promete menor tempo de execução em relação ao algoritmo original.